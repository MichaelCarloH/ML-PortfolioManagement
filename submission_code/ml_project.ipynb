{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a0b376",
   "metadata": {},
   "source": [
    "# Notebook initialization\n",
    "\n",
    "Run this cell to create all helper files and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1bf3ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michael\\Code\\ML-PortfolioManagement\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pandas numpy matplotlib seaborn requests scikit-learn tqdm pyarrow --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39848d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_processing.py\n",
    "\"\"\"\n",
    "data_processing.py\n",
    "\n",
    "Utility functions for:\n",
    "- Loading and processing Fed-related CSV files into a single ML-ready DataFrame\n",
    "- Optionally downloading Fed CSV files from GitHub into ./data/fed_csv\n",
    "- Loading and processing S&P 500 OHLCV data\n",
    "\n",
    "This module is generated dynamically inside the notebook\n",
    "to comply with the \"no external .py files\" requirement.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Local directory where Fed CSV files are stored\n",
    "DATA_DIR = \"data/fed_csv\"\n",
    "\n",
    "# GitHub repo configuration (optional, for reproducibility)\n",
    "REPO_OWNER = \"MichaelCarloH\"\n",
    "REPO_NAME = \"ML-PortfolioManagement\"\n",
    "FOLDER_PATH = \"data/fed_csv\"  # path inside the repo (no leading/trailing slash)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def _log(msg: str, verbose: bool) -> None:\n",
    "    if verbose:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "def ensure_data_dir(path: str = DATA_DIR) -> None:\n",
    "    \"\"\"\n",
    "    Ensure that the local data directory exists.\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def fetch_csv_from_github_folder(\n",
    "    repo_owner: str = REPO_OWNER,\n",
    "    repo_name: str = REPO_NAME,\n",
    "    folder_path: str = FOLDER_PATH,\n",
    "    local_dir: str = DATA_DIR,\n",
    "    verbose: bool = False,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Try to fetch all .csv files from a GitHub folder via the GitHub API\n",
    "    and save them into `local_dir`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if at least one CSV was downloaded, False otherwise.\n",
    "    \"\"\"\n",
    "    ensure_data_dir(local_dir)\n",
    "\n",
    "    api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{folder_path}\"\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        _log(\n",
    "            f\"[INFO] Could not access GitHub folder ({response.status_code}). \"\n",
    "            \"Proceeding without remote download.\",\n",
    "            verbose,\n",
    "        )\n",
    "        return False\n",
    "\n",
    "    items = response.json()\n",
    "    csv_files = [item for item in items if item[\"name\"].endswith(\".csv\")]\n",
    "\n",
    "    if not csv_files:\n",
    "        _log(\"[INFO] No CSV files found in the specified GitHub folder.\", verbose)\n",
    "        return False\n",
    "\n",
    "    downloaded_any = False\n",
    "    for item in csv_files:\n",
    "        download_url = item[\"download_url\"]\n",
    "        filename = item[\"name\"]\n",
    "        local_path = os.path.join(local_dir, filename)\n",
    "\n",
    "        file_response = requests.get(download_url)\n",
    "        if file_response.status_code == 200:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(file_response.content)\n",
    "            downloaded_any = True\n",
    "            _log(f\"Downloaded: {filename}\", verbose)\n",
    "        else:\n",
    "            _log(\n",
    "                f\"[WARN] Failed to download {filename}: {file_response.status_code}\",\n",
    "                verbose,\n",
    "            )\n",
    "\n",
    "    return downloaded_any\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Fed CSV processing\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def load_all_csvs(\n",
    "    local_dir: str = DATA_DIR,\n",
    "    pattern: str = \"*.csv\",\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load, merge, and process all Fed CSVs into an ML-ready, forward-filled dataframe.\n",
    "\n",
    "    Steps:\n",
    "    - Reads all CSVs in `local_dir` matching `pattern`.\n",
    "    - Extracts date and outcome columns.\n",
    "    - Merges all files into one unified dataframe.\n",
    "    - Converts to ML-wide format with clean feature names.\n",
    "    - Saves:\n",
    "        - data/fed_events_merged.csv\n",
    "        - data/fed_events_ml_ready.csv\n",
    "        - data/fed_events_ml_ready_ffill.csv\n",
    "    - Returns:\n",
    "        - Forward-filled ML-ready DataFrame (fed_events_ml_ready_ffill.csv).\n",
    "    \"\"\"\n",
    "\n",
    "    def log(msg: str):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "\n",
    "    ensure_data_dir(local_dir)\n",
    "\n",
    "    # --- Step 1: find CSV files ---\n",
    "    fed_csv_path = Path(local_dir)\n",
    "    csv_files = list(fed_csv_path.glob(pattern))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {fed_csv_path} matching {pattern}\")\n",
    "    log(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "    # --- Step 2: process each file ---\n",
    "    all_dataframes = []\n",
    "    for csv_file in csv_files:\n",
    "        title = csv_file.stem\n",
    "        log(f\"Processing: {title}\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "\n",
    "            # Identify date column\n",
    "            date_col = next(\n",
    "                (c for c in df.columns if \"date\" in c.lower() and \"utc\" in c.lower()),\n",
    "                None,\n",
    "            )\n",
    "            if date_col is None:\n",
    "                log(f\"  ⚠ No date column found, skipping {title}\")\n",
    "                continue\n",
    "\n",
    "            # Identify outcome columns\n",
    "            outcome_cols = [\n",
    "                c for c in df.columns\n",
    "                if c != date_col and \"timestamp\" not in c.lower()\n",
    "            ]\n",
    "            if not outcome_cols:\n",
    "                log(f\"  ⚠ No outcome columns found, skipping {title}\")\n",
    "                continue\n",
    "\n",
    "            # Convert data types\n",
    "            df[date_col] = pd.to_datetime(\n",
    "                df[date_col],\n",
    "                format=\"%m-%d-%Y %H:%M\",\n",
    "                errors=\"coerce\",\n",
    "            )\n",
    "            for c in outcome_cols:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "            # Merge same-date rows: first non-null per column\n",
    "            merged_df = df.groupby(date_col).agg({\n",
    "                c: (lambda x: x.dropna().iloc[0] if len(x.dropna()) > 0 else None)\n",
    "                for c in outcome_cols\n",
    "            }).reset_index()\n",
    "\n",
    "            merged_df[\"Title\"] = title\n",
    "            merged_df = merged_df.rename(columns={date_col: \"Date\"})\n",
    "            merged_df = merged_df[[\"Date\", \"Title\"] + outcome_cols]\n",
    "            all_dataframes.append(merged_df)\n",
    "            log(f\"  ✓ {len(merged_df)} unique dates\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"  ✗ Error processing {title}: {e}\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        raise RuntimeError(\"No valid CSVs processed.\")\n",
    "\n",
    "    # --- Step 3: merge all dataframes ---\n",
    "    log(\"Merging all dataframes...\")\n",
    "    all_outcome_cols = sorted(list({\n",
    "        c\n",
    "        for df in all_dataframes\n",
    "        for c in df.columns\n",
    "        if c not in [\"Date\", \"Title\"]\n",
    "    }))\n",
    "\n",
    "    final_dfs = []\n",
    "    for df in all_dataframes:\n",
    "        for c in all_outcome_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = None\n",
    "        final_dfs.append(df[[\"Date\", \"Title\"] + all_outcome_cols])\n",
    "\n",
    "    final_df = (\n",
    "        pd.concat(final_dfs, ignore_index=True)\n",
    "        .sort_values([\"Date\", \"Title\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    merged_path = Path(\"data/fed_events_merged.csv\")\n",
    "    final_df.to_csv(merged_path, index=False)\n",
    "    log(f\"Saved merged dataframe to: {merged_path}\")\n",
    "\n",
    "    # --- Step 4: long format ---\n",
    "    id_vars = [\"Date\", \"Title\"]\n",
    "    value_cols = [c for c in final_df.columns if c not in id_vars]\n",
    "\n",
    "    df_long = pd.melt(\n",
    "        final_df,\n",
    "        id_vars=id_vars,\n",
    "        value_vars=value_cols,\n",
    "        var_name=\"Outcome\",\n",
    "        value_name=\"Probability\",\n",
    "    ).dropna(subset=[\"Probability\"])\n",
    "\n",
    "    # --- Step 5: clean feature names ---\n",
    "    def sanitize_feature_name(title: str, outcome: str) -> str:\n",
    "        feature = f\"{title}_{outcome}\"\n",
    "        feature = re.sub(r\"[^a-zA-Z0-9_\\s]\", \"\", feature)\n",
    "        feature = re.sub(r\"\\s+\", \"_\", feature)\n",
    "        feature = re.sub(r\"_+\", \"_\", feature).strip(\"_\")\n",
    "        return feature\n",
    "\n",
    "    df_long[\"Feature\"] = df_long.apply(\n",
    "        lambda r: sanitize_feature_name(r[\"Title\"], r[\"Outcome\"]),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # --- Step 6: pivot to ML-wide ---\n",
    "    df_ml = df_long.pivot_table(\n",
    "        index=\"Date\",\n",
    "        columns=\"Feature\",\n",
    "        values=\"Probability\",\n",
    "        aggfunc=\"first\",\n",
    "    ).reset_index()\n",
    "\n",
    "    cols = [\"Date\"] + sorted([c for c in df_ml.columns if c != \"Date\"])\n",
    "    df_ml = df_ml[cols]\n",
    "\n",
    "    ml_ready_path = Path(\"data/fed_events_ml_ready.csv\")\n",
    "    df_ml.to_csv(ml_ready_path, index=False)\n",
    "    log(f\"Saved ML-ready dataframe to: {ml_ready_path}\")\n",
    "\n",
    "    # --- Step 7: forward-fill version ---\n",
    "    df_ml_ffill = df_ml.sort_values(\"Date\").copy()\n",
    "    df_ml_ffill.iloc[:, 1:] = df_ml_ffill.iloc[:, 1:].ffill().fillna(0)\n",
    "\n",
    "    ffill_path = Path(\"data/fed_events_ml_ready_ffill.csv\")\n",
    "    df_ml_ffill.to_csv(ffill_path, index=False)\n",
    "    log(f\"Saved forward-filled dataframe to: {ffill_path}\")\n",
    "\n",
    "    # --- Step 8: summary (optional) ---\n",
    "    log(\"=== FED DATA SUMMARY ===\")\n",
    "    log(f\"Rows (dates): {len(df_ml_ffill)}\")\n",
    "    log(f\"Features: {len(df_ml_ffill.columns) - 1}\")\n",
    "    log(\n",
    "        f\"Date range: {df_ml_ffill['Date'].min()} → {df_ml_ffill['Date'].max()}\"\n",
    "    )\n",
    "\n",
    "    return df_ml_ffill\n",
    "\n",
    "\n",
    "def get_fed_data(verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    High-level convenience function for the notebook.\n",
    "\n",
    "    - Tries to load processed data from local ./data/fed_csv.\n",
    "    - If no raw CSVs exist, optionally tries to download them from GitHub.\n",
    "    - Returns the final forward-filled ML-ready dataframe.\n",
    "    \"\"\"\n",
    "    ensure_data_dir(DATA_DIR)\n",
    "\n",
    "    # 1) Check for existing local CSVs\n",
    "    csv_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
    "\n",
    "    # 2) If none, try GitHub (non-fatal if it fails)\n",
    "    if not csv_files:\n",
    "        _log(\n",
    "            \"No local Fed CSV files found in ./data/fed_csv. \"\n",
    "            \"Attempting to download from GitHub...\",\n",
    "            verbose,\n",
    "        )\n",
    "        ok = fetch_csv_from_github_folder(verbose=verbose)\n",
    "        if not ok:\n",
    "            raise FileNotFoundError(\n",
    "                \"No local Fed CSVs and GitHub download failed. \"\n",
    "                \"Ensure ./data/fed_csv contains the required files \"\n",
    "                \"in the submitted project.\"\n",
    "            )\n",
    "\n",
    "    # 3) Build and return processed dataframe\n",
    "    return load_all_csvs(local_dir=DATA_DIR, verbose=verbose)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# S&P 500 loader\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def load_sp500_data(\n",
    "    ticker: str = \"^GSPC\",\n",
    "    start: str = \"2023-01-01\",\n",
    "    end: str = None,\n",
    "    output_path: str = \"data/sp500_ohlcv_returns.csv\",\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download, process, and save S&P 500 OHLCV and derived metrics.\n",
    "\n",
    "    Returns a dataframe indexed by Date (UTC) with:\n",
    "    - OHLCV\n",
    "    - Daily_Return, Log_Return\n",
    "    - High/Low and Open/Close ranges\n",
    "    - Volume_MA_20, Volume_Ratio\n",
    "    - Price_MA_20, Price_MA_50\n",
    "    - Volatility_20\n",
    "    \"\"\"\n",
    "\n",
    "    def log(msg: str):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log(f\"Downloading {ticker} from Yahoo Finance...\")\n",
    "    spx = yf.download(\n",
    "        ticker,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        progress=False,\n",
    "        auto_adjust=True,\n",
    "    )\n",
    "\n",
    "    if spx.empty:\n",
    "        raise RuntimeError(f\"No data returned for {ticker} from Yahoo Finance.\")\n",
    "\n",
    "    # Flatten multi-index columns if present\n",
    "    if isinstance(spx.columns, pd.MultiIndex):\n",
    "        spx.columns = spx.columns.get_level_values(0)\n",
    "\n",
    "    # Metrics\n",
    "    spx[\"Daily_Return\"] = spx[\"Close\"].pct_change()\n",
    "    spx[\"Log_Return\"] = np.log(spx[\"Close\"] / spx[\"Close\"].shift(1))\n",
    "    spx[\"High_Low_Range\"] = (spx[\"High\"] - spx[\"Low\"]) / spx[\"Close\"]\n",
    "    spx[\"Open_Close_Range\"] = (spx[\"Open\"] - spx[\"Close\"]).abs() / spx[\"Close\"]\n",
    "    spx[\"Volume_MA_20\"] = spx[\"Volume\"].rolling(20).mean()\n",
    "    spx[\"Volume_Ratio\"] = spx[\"Volume\"] / spx[\"Volume_MA_20\"]\n",
    "    spx[\"Price_MA_20\"] = spx[\"Close\"].rolling(20).mean()\n",
    "    spx[\"Price_MA_50\"] = spx[\"Close\"].rolling(50).mean()\n",
    "    spx[\"Volatility_20\"] = spx[\"Daily_Return\"].rolling(20).std()\n",
    "\n",
    "    cols_to_keep = [\n",
    "        \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n",
    "        \"Daily_Return\", \"Log_Return\",\n",
    "        \"High_Low_Range\", \"Open_Close_Range\",\n",
    "        \"Volume_MA_20\", \"Volume_Ratio\",\n",
    "        \"Price_MA_20\", \"Price_MA_50\", \"Volatility_20\",\n",
    "    ]\n",
    "    available = [c for c in cols_to_keep if c in spx.columns]\n",
    "\n",
    "    daily = spx[available].dropna().copy()\n",
    "    daily.index.name = \"Date (UTC)\"\n",
    "\n",
    "    daily.to_csv(output_path)\n",
    "    log(f\"Saved S&P 500 data to: {output_path.resolve()}\")\n",
    "\n",
    "    if verbose:\n",
    "        log(f\"Rows: {len(daily)}\")\n",
    "        log(f\"Date range: {daily.index.min()} → {daily.index.max()}\")\n",
    "\n",
    "    return daily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8660f7",
   "metadata": {},
   "source": [
    "# Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b2d86",
   "metadata": {},
   "source": [
    "We outline a structured approach for presenting research findings. The framework is divided into several key segments:\n",
    "\n",
    "1. Introduction\n",
    "1. Dataset overview\n",
    "1. Analytics and learning strategies\n",
    "1. Empirical resuts: baseline and robustness \n",
    "1. Conclusion\n",
    "\n",
    "The opening segment encompasses four essential elements:\n",
    "\n",
    "- Contextual Background: What is the larger setting of the study? What makes this area of inquiry compelling? What are the existing gaps or limitations within the current body of research? What are some unanswered yet noteworthy questions?\n",
    "\n",
    "- Project Contributions: What are the specific advancements made by this study, such as in data acquisition, algorithmic development, parameter adjustments, etc.?\n",
    "\n",
    "- Summary of the main empirical results: What is the main statistical statement? is it significant (e.g. statistically or economically)? \n",
    "\n",
    "- Literature and Resource Citations: What are related academic papers? What are the github repositories, expert blogs, or software packages that used in this project? \n",
    "\n",
    "In the dataset profile, one should consider:\n",
    "\n",
    "- The origin and composition of data utilized in the study. If the dataset is original, then provide the source code to ensure reproducibility.\n",
    "\n",
    "- The chronological accuracy of the data points, verifying that the dates reflect the actual availability of information.\n",
    "\n",
    "- A detailed analysis of descriptive statistics, with an emphasis on discussing the importance of the chosen graphs or metrics.\n",
    "\n",
    "The analytics and machine learning methodologies section accounts for:\n",
    "\n",
    "- A detailed explanation of the foundational algorithm.\n",
    "\n",
    "- A description of the data partitioning strategy for training, validation and test.\n",
    "\n",
    "- An overview of the parameter selection and optimization process.\n",
    "\n",
    "To effectively convey the empirical findings, separate the baseline results from the additional robustness tests. Within the primary empirical outcomes portion, include:\n",
    "\n",
    "- Key statistical evaluations (for instance, if presenting a backtest – provide a pnl graph alongside the Sharpe ratio).\n",
    "\n",
    "- Insights into what primarily influences the results, such as specific characteristics or assets that significantly impact performance.\n",
    "\n",
    "The robustness of empirical tests section should detail:\n",
    "\n",
    "- Evaluation of the stability of the principal finding against variations in hyperparameters or algorithmic modifications.\n",
    "\n",
    "Finally, the conclusive synthesis should recapitulate the primary findings, consider external elements that may influence the results, and hint at potential directions for further investigative work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5a3c4",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ed16d",
   "metadata": {},
   "source": [
    "In this project of Machine Learning for Portfolio Managment and Trading, we are tasked to loosely investigate the effect of central banks policies on the market. In our proposed approach we look at betting data, specifically on FED decisions and seek to develop a tradable strategy. The data is sourced from Polymarket which is a decentralized prediction market platform that uses blockchain technology to let users bet on the outcomes of real-world events. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a36af9",
   "metadata": {},
   "source": [
    "## Contextual Background\n",
    "- **Macro-financial setting:** The project operates at the intersection of monetary-policy expectations and equity-market positioning, focusing on how Federal Open Market Committee (FOMC) decisions ripple through the S&P 500. Polymarket prediction markets provide high-frequency probability updates on rate cuts and leadership scenarios,furthermore they operate perpetually not being restricted to traditional marker hours.\n",
    "- **Motivation and gaps:** Prior academic and practitioner research documents that option-implied probabilities and macro-news sentiment foreshadow FOMC-day volatility, yet public prediction-market data remains underexplored. Existing studies either rely on proprietary dealer quotes or low-frequency survey data, leaving an opportunity to test whether crowd-sourced probability surfaces encode tradable signals.\n",
    "- **Open questions:** Key open questions include (i) how quickly prediction-market probabilities anticipate rate moves, (ii) whether the implied probabilities add incremental signal once traditional macro data is controlled for, and (iii) the robustness of any trading edge once transaction costs and alternate market regimes are considered.\n",
    "\n",
    "## Project Contributions\n",
    "- **Integrated data acquisition:** Consolidated raw Polymarket CSV exports and manually curated Federal Reserve event datasets into harmonised, analysis-ready tables (e.g., `polymarket_fed_events_and_markets.csv`, `fed_events_ml_ready_ffill.csv`).\n",
    "- **Feature engineering pipeline:** Produced merged S&P 500 and Polymarket features with forward-filled probability fields, calendar event encodings, and target variables capturing post-announcement returns (`sp500_fed_ml_ready_with_targets.csv`).\n",
    "- **Model experimentation:** Implemented data loaders (`dataloaders/s&p_loader.py`) and notebooks (`notebooks/fed_probability_prediction.ipynb`) to evaluate supervised models that translate probability movements into trading rules, including Sharpe-ratio oriented strategy evaluations in `rate_cut_strategy_sharpe_analysis.csv` outputs.\n",
    "- **Backtesting utilities:** Generated out-of-sample trade logs (`notebooks/outputs/trades_oos.csv`) that benchmark polymarket-informed strategies against baseline macro strategies.\n",
    "\n",
    "## Summary of Empirical Results\n",
    "- **Performance snapshot:** The `notebooks/outputs/trades_all_strategies.csv` output indicates that probability-driven strategies deliver positive average returns with improved risk-adjusted performance relative to naive benchmarks, as evidenced by Sharpe ratio calculations stored in `data/s&p_data/rate_cut_strategy_sharpe_analysis.csv`.\n",
    "- **Economic significance:** Strategy evaluations demonstrate economically meaningful improvements in Sharpe ratios when prediction-market features are incorporated, highlighting the potential of crowd-sourced expectations to complement conventional macro indicators.\n",
    "\n",
    "## Literature and Resource Citations\n",
    "- **Related research:**\n",
    "  - Andersen, T. G., Bollerslev, T., Diebold, F. X., & Vega, C. (2007). *Real-Time Price Discovery in Stock, Bond and Foreign Exchange Markets*.\n",
    "  - Lucca, D. O., & Moench, E. (2015). *The Pre-FOMC Announcement Drift*.\n",
    "  - Wolfers, J., & Zitzewitz, E. (2006). *Prediction Markets in Theory and Practice*.\n",
    "- **Software and repositories:**\n",
    "  - [Polymarket API & CSV exports](https://polymarket.com/)\n",
    "  - [Pandas](https://pandas.pydata.org/), [scikit-learn](https://scikit-learn.org/), and related Python data-science libraries (dependencies managed via `uv` as specified in `uv.toml`).\n",
    "\n",
    "## Dataset Profile\n",
    "- **Source composition:**\n",
    "  - **Polymarket market histories:** Stored under `data/s&p_data/`, including `polymarket-price-data-24-07-2025-24-10-2025-1761316761596.csv` and the merged `polymarket_fed_events_and_markets.csv` file.\n",
    "  - **Federal Reserve event catalogues:** CSV files in `data/fed_csv/` and curated aggregations such as `fed_events_merged.csv`, providing policy-meeting metadata and outcome annotations.\n",
    "  - **Market benchmarks:** S&P 500 price and returns series within `data/s&p_data/sp500_ohlcv_returns.csv` and derived merges (`sp500_fed_merged_ml_ready.csv`).\n",
    "- **Processing workflow:** The `notebooks/data_exploration.ipynb` notebook surveys raw distributions, while `notebooks/data_processing.ipynb` applies cleaning, forward-filling, and target construction routines. Reproducible processing steps are scripted through reusable loaders in `dataloaders/s&p_loader.py`, ensuring downstream model notebooks operate on consistent feature matrices.\n",
    "- **Reproducibility notes:** Raw Polymarket CSVs and Fed event files are preserved in the repository. Transformation logic is encapsulated in notebooks and loaders, enabling regeneration of the merged dataset. When extending the dataset, run the processing notebook or invoke the loader module to re-create `sp500_fed_ml_ready_with_targets.csv` from the raw components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8030ac",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dee4ad",
   "metadata": {},
   "source": [
    "The data used is Polymarket betting quotes on several Fed related topics, csv data can be downloaded at: https://polymarket.com/search?_q=fed\n",
    "\n",
    "In this notebook we point to the Github repository where static data obtained at the time of the analysis is located and can be used to reproduce the results of this project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25891bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michael\\Code\\ML-PortfolioManagement\\submission_code\\data_processing.py:218: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pd.concat(final_dfs, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Feature</th>\n",
       "      <th>Date</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_25_bps_decrease</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_25_bps_increase</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_50_bps_decrease</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_75_bps_decrease</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_No_Change</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_Other</th>\n",
       "      <th>Fed_abolished_in_2025_Price</th>\n",
       "      <th>Fed_decision_in_December_25_bps_decrease</th>\n",
       "      <th>Fed_decision_in_December_25_bps_increase</th>\n",
       "      <th>...</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Larry_Lindsey</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Lorie_K_Logan</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Marc_Sumerlin</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Michelle_Bowman</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_No_one_nominated_before_2027</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Philip_Jefferson</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Rick_Rieder</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Ron_Paul</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Scott_Bessent</th>\n",
       "      <th>Who_will_Trump_nominate_as_Fed_Chair_Stephen_Miran</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-03</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-04</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-06</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-07</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Feature       Date  Fed_Interest_Rates_November_2024_25_bps_decrease  \\\n",
       "0       2024-08-03                                             0.315   \n",
       "1       2024-08-04                                             0.370   \n",
       "2       2024-08-05                                             0.355   \n",
       "3       2024-08-06                                             0.345   \n",
       "4       2024-08-07                                             0.380   \n",
       "\n",
       "Feature  Fed_Interest_Rates_November_2024_25_bps_increase  \\\n",
       "0                                                  0.0250   \n",
       "1                                                  0.0225   \n",
       "2                                                  0.0225   \n",
       "3                                                  0.0110   \n",
       "4                                                  0.0125   \n",
       "\n",
       "Feature  Fed_Interest_Rates_November_2024_50_bps_decrease  \\\n",
       "0                                                   0.270   \n",
       "1                                                   0.265   \n",
       "2                                                   0.275   \n",
       "3                                                   0.325   \n",
       "4                                                   0.275   \n",
       "\n",
       "Feature  Fed_Interest_Rates_November_2024_75_bps_decrease  \\\n",
       "0                                                   0.125   \n",
       "1                                                   0.125   \n",
       "2                                                   0.125   \n",
       "3                                                   0.110   \n",
       "4                                                   0.105   \n",
       "\n",
       "Feature  Fed_Interest_Rates_November_2024_No_Change  \\\n",
       "0                                             0.215   \n",
       "1                                             0.215   \n",
       "2                                             0.200   \n",
       "3                                             0.170   \n",
       "4                                             0.130   \n",
       "\n",
       "Feature  Fed_Interest_Rates_November_2024_Other  Fed_abolished_in_2025_Price  \\\n",
       "0                                           0.0                          0.0   \n",
       "1                                           0.0                          0.0   \n",
       "2                                           0.0                          0.0   \n",
       "3                                           0.0                          0.0   \n",
       "4                                           0.0                          0.0   \n",
       "\n",
       "Feature  Fed_decision_in_December_25_bps_decrease  \\\n",
       "0                                             0.0   \n",
       "1                                             0.0   \n",
       "2                                             0.0   \n",
       "3                                             0.0   \n",
       "4                                             0.0   \n",
       "\n",
       "Feature  Fed_decision_in_December_25_bps_increase  ...  \\\n",
       "0                                             0.0  ...   \n",
       "1                                             0.0  ...   \n",
       "2                                             0.0  ...   \n",
       "3                                             0.0  ...   \n",
       "4                                             0.0  ...   \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Larry_Lindsey  \\\n",
       "0                                                      0.0    \n",
       "1                                                      0.0    \n",
       "2                                                      0.0    \n",
       "3                                                      0.0    \n",
       "4                                                      0.0    \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Lorie_K_Logan  \\\n",
       "0                                                      0.0    \n",
       "1                                                      0.0    \n",
       "2                                                      0.0    \n",
       "3                                                      0.0    \n",
       "4                                                      0.0    \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Marc_Sumerlin  \\\n",
       "0                                                      0.0    \n",
       "1                                                      0.0    \n",
       "2                                                      0.0    \n",
       "3                                                      0.0    \n",
       "4                                                      0.0    \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Michelle_Bowman  \\\n",
       "0                                                      0.0      \n",
       "1                                                      0.0      \n",
       "2                                                      0.0      \n",
       "3                                                      0.0      \n",
       "4                                                      0.0      \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_No_one_nominated_before_2027  \\\n",
       "0                                                      0.0                   \n",
       "1                                                      0.0                   \n",
       "2                                                      0.0                   \n",
       "3                                                      0.0                   \n",
       "4                                                      0.0                   \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Philip_Jefferson  \\\n",
       "0                                                      0.0       \n",
       "1                                                      0.0       \n",
       "2                                                      0.0       \n",
       "3                                                      0.0       \n",
       "4                                                      0.0       \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Rick_Rieder  \\\n",
       "0                                                     0.0   \n",
       "1                                                     0.0   \n",
       "2                                                     0.0   \n",
       "3                                                     0.0   \n",
       "4                                                     0.0   \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Ron_Paul  \\\n",
       "0                                                  0.0   \n",
       "1                                                  0.0   \n",
       "2                                                  0.0   \n",
       "3                                                  0.0   \n",
       "4                                                  0.0   \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Scott_Bessent  \\\n",
       "0                                                      0.0    \n",
       "1                                                      0.0    \n",
       "2                                                      0.0    \n",
       "3                                                      0.0    \n",
       "4                                                      0.0    \n",
       "\n",
       "Feature  Who_will_Trump_nominate_as_Fed_Chair_Stephen_Miran  \n",
       "0                                                      0.0   \n",
       "1                                                      0.0   \n",
       "2                                                      0.0   \n",
       "3                                                      0.0   \n",
       "4                                                      0.0   \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from data_processing import get_fed_data\n",
    "\n",
    "fed_df = get_fed_data()\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332557d",
   "metadata": {},
   "source": [
    "# Merging Fed time series with S&P500 time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b81c77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import load_sp500_data\n",
    "spx = load_sp500_data(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139324a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_25_bps_decrease</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_25_bps_increase</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_50_bps_decrease</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_75_bps_decrease</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_No_Change</th>\n",
       "      <th>Fed_Interest_Rates_November_2024_Other</th>\n",
       "      <th>Fed_abolished_in_2025_Price</th>\n",
       "      <th>Fed_decision_in_December_25_bps_decrease</th>\n",
       "      <th>Fed_decision_in_December_25_bps_increase</th>\n",
       "      <th>...</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Daily_Return</th>\n",
       "      <th>Log_Return</th>\n",
       "      <th>High_Low_Range</th>\n",
       "      <th>Open_Close_Range</th>\n",
       "      <th>Volume_MA_20</th>\n",
       "      <th>Volume_Ratio</th>\n",
       "      <th>Price_MA_20</th>\n",
       "      <th>Price_MA_50</th>\n",
       "      <th>Volatility_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-15 00:00:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.594010e+09</td>\n",
       "      <td>-0.006981</td>\n",
       "      <td>-0.007005</td>\n",
       "      <td>0.014394</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>4.499821e+09</td>\n",
       "      <td>1.465394</td>\n",
       "      <td>3984.615515</td>\n",
       "      <td>4003.584209</td>\n",
       "      <td>0.010362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-16 00:00:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.695790e+09</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>0.017410</td>\n",
       "      <td>0.025339</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>4.580812e+09</td>\n",
       "      <td>1.243402</td>\n",
       "      <td>3975.249512</td>\n",
       "      <td>4006.307012</td>\n",
       "      <td>0.011282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-17 00:00:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.354280e+09</td>\n",
       "      <td>-0.011019</td>\n",
       "      <td>-0.011081</td>\n",
       "      <td>0.014717</td>\n",
       "      <td>0.010736</td>\n",
       "      <td>4.841342e+09</td>\n",
       "      <td>1.932167</td>\n",
       "      <td>3966.561011</td>\n",
       "      <td>4007.580410</td>\n",
       "      <td>0.011149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-20 00:00:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.347140e+09</td>\n",
       "      <td>0.008918</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>4.906426e+09</td>\n",
       "      <td>1.089824</td>\n",
       "      <td>3960.185010</td>\n",
       "      <td>4010.449810</td>\n",
       "      <td>0.011415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-21 00:00:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.920240e+09</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.012899</td>\n",
       "      <td>0.009466</td>\n",
       "      <td>0.006740</td>\n",
       "      <td>4.946358e+09</td>\n",
       "      <td>0.994720</td>\n",
       "      <td>3960.461511</td>\n",
       "      <td>4012.605610</td>\n",
       "      <td>0.010976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>2025-11-06 17:47:00</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>2025-11-06 17:48:00</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>2025-11-06 17:49:00</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>2025-11-06 17:50:00</td>\n",
       "      <td>0.9535</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>2025-11-07 00:00:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.926070e+09</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>5.685095e+09</td>\n",
       "      <td>1.042387</td>\n",
       "      <td>6757.564478</td>\n",
       "      <td>6670.289580</td>\n",
       "      <td>0.007516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>806 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date  Fed_Interest_Rates_November_2024_25_bps_decrease  \\\n",
       "0   2023-03-15 00:00:00                                            0.0000   \n",
       "1   2023-03-16 00:00:00                                            0.0000   \n",
       "2   2023-03-17 00:00:00                                            0.0000   \n",
       "3   2023-03-20 00:00:00                                            0.0000   \n",
       "4   2023-03-21 00:00:00                                            0.0000   \n",
       "..                  ...                                               ...   \n",
       "801 2025-11-06 17:47:00                                            0.9535   \n",
       "802 2025-11-06 17:48:00                                            0.9535   \n",
       "803 2025-11-06 17:49:00                                            0.9535   \n",
       "804 2025-11-06 17:50:00                                            0.9535   \n",
       "805 2025-11-07 00:00:00                                            0.0000   \n",
       "\n",
       "     Fed_Interest_Rates_November_2024_25_bps_increase  \\\n",
       "0                                               0.000   \n",
       "1                                               0.000   \n",
       "2                                               0.000   \n",
       "3                                               0.000   \n",
       "4                                               0.000   \n",
       "..                                                ...   \n",
       "801                                             0.003   \n",
       "802                                             0.003   \n",
       "803                                             0.003   \n",
       "804                                             0.003   \n",
       "805                                             0.000   \n",
       "\n",
       "     Fed_Interest_Rates_November_2024_50_bps_decrease  \\\n",
       "0                                               0.000   \n",
       "1                                               0.000   \n",
       "2                                               0.000   \n",
       "3                                               0.000   \n",
       "4                                               0.000   \n",
       "..                                                ...   \n",
       "801                                             0.021   \n",
       "802                                             0.021   \n",
       "803                                             0.021   \n",
       "804                                             0.021   \n",
       "805                                             0.000   \n",
       "\n",
       "     Fed_Interest_Rates_November_2024_75_bps_decrease  \\\n",
       "0                                               0.000   \n",
       "1                                               0.000   \n",
       "2                                               0.000   \n",
       "3                                               0.000   \n",
       "4                                               0.000   \n",
       "..                                                ...   \n",
       "801                                             0.003   \n",
       "802                                             0.003   \n",
       "803                                             0.003   \n",
       "804                                             0.003   \n",
       "805                                             0.000   \n",
       "\n",
       "     Fed_Interest_Rates_November_2024_No_Change  \\\n",
       "0                                         0.000   \n",
       "1                                         0.000   \n",
       "2                                         0.000   \n",
       "3                                         0.000   \n",
       "4                                         0.000   \n",
       "..                                          ...   \n",
       "801                                       0.025   \n",
       "802                                       0.025   \n",
       "803                                       0.025   \n",
       "804                                       0.025   \n",
       "805                                       0.000   \n",
       "\n",
       "     Fed_Interest_Rates_November_2024_Other  Fed_abolished_in_2025_Price  \\\n",
       "0                                    0.0000                         0.00   \n",
       "1                                    0.0000                         0.00   \n",
       "2                                    0.0000                         0.00   \n",
       "3                                    0.0000                         0.00   \n",
       "4                                    0.0000                         0.00   \n",
       "..                                      ...                          ...   \n",
       "801                                  0.0015                         0.01   \n",
       "802                                  0.0015                         0.01   \n",
       "803                                  0.0015                         0.01   \n",
       "804                                  0.0015                         0.01   \n",
       "805                                  0.0000                         0.00   \n",
       "\n",
       "     Fed_decision_in_December_25_bps_decrease  \\\n",
       "0                                       0.000   \n",
       "1                                       0.000   \n",
       "2                                       0.000   \n",
       "3                                       0.000   \n",
       "4                                       0.000   \n",
       "..                                        ...   \n",
       "801                                     0.715   \n",
       "802                                     0.715   \n",
       "803                                     0.715   \n",
       "804                                     0.715   \n",
       "805                                     0.000   \n",
       "\n",
       "     Fed_decision_in_December_25_bps_increase  ...        Volume  \\\n",
       "0                                      0.0000  ...  6.594010e+09   \n",
       "1                                      0.0000  ...  5.695790e+09   \n",
       "2                                      0.0000  ...  9.354280e+09   \n",
       "3                                      0.0000  ...  5.347140e+09   \n",
       "4                                      0.0000  ...  4.920240e+09   \n",
       "..                                        ...  ...           ...   \n",
       "801                                    0.0125  ...  0.000000e+00   \n",
       "802                                    0.0125  ...  0.000000e+00   \n",
       "803                                    0.0125  ...  0.000000e+00   \n",
       "804                                    0.0125  ...  0.000000e+00   \n",
       "805                                    0.0000  ...  5.926070e+09   \n",
       "\n",
       "     Daily_Return  Log_Return  High_Low_Range  Open_Close_Range  Volume_MA_20  \\\n",
       "0       -0.006981   -0.007005        0.014394          0.003903  4.499821e+09   \n",
       "1        0.017562    0.017410        0.025339          0.020542  4.580812e+09   \n",
       "2       -0.011019   -0.011081        0.014717          0.010736  4.841342e+09   \n",
       "3        0.008918    0.008879        0.010054          0.008630  4.906426e+09   \n",
       "4        0.012982    0.012899        0.009466          0.006740  4.946358e+09   \n",
       "..            ...         ...             ...               ...           ...   \n",
       "801      0.000000    0.000000        0.000000          0.000000  0.000000e+00   \n",
       "802      0.000000    0.000000        0.000000          0.000000  0.000000e+00   \n",
       "803      0.000000    0.000000        0.000000          0.000000  0.000000e+00   \n",
       "804      0.000000    0.000000        0.000000          0.000000  0.000000e+00   \n",
       "805      0.001262    0.001261        0.014664          0.004848  5.685095e+09   \n",
       "\n",
       "     Volume_Ratio  Price_MA_20  Price_MA_50  Volatility_20  \n",
       "0        1.465394  3984.615515  4003.584209       0.010362  \n",
       "1        1.243402  3975.249512  4006.307012       0.011282  \n",
       "2        1.932167  3966.561011  4007.580410       0.011149  \n",
       "3        1.089824  3960.185010  4010.449810       0.011415  \n",
       "4        0.994720  3960.461511  4012.605610       0.010976  \n",
       "..            ...          ...          ...            ...  \n",
       "801      0.000000     0.000000     0.000000       0.000000  \n",
       "802      0.000000     0.000000     0.000000       0.000000  \n",
       "803      0.000000     0.000000     0.000000       0.000000  \n",
       "804      0.000000     0.000000     0.000000       0.000000  \n",
       "805      1.042387  6757.564478  6670.289580       0.007516  \n",
       "\n",
       "[806 rows x 130 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize S&P 500 date column\n",
    "spx = spx.reset_index().rename(columns={\"Date (UTC)\": \"Date\"})\n",
    "fed_df[\"Date\"] = pd.to_datetime(fed_df[\"Date\"])\n",
    "spx[\"Date\"] = pd.to_datetime(spx[\"Date\"])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Full outer merge on Date (keep all dates)\n",
    "# --------------------------------------------------\n",
    "merged_df = pd.merge(\n",
    "    fed_df,\n",
    "    spx,\n",
    "    on=\"Date\",\n",
    "    how=\"outer\",    # full join\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "# Fill all non-date missing values with zeros\n",
    "feature_cols = [c for c in merged_df.columns if c != \"Date\"]\n",
    "merged_df[feature_cols] = merged_df[feature_cols].fillna(0)\n",
    "\n",
    "# Sort chronologically\n",
    "merged_df = merged_df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac33d9",
   "metadata": {},
   "source": [
    "# Indicator fo fed market sentiment shift\n",
    "\n",
    "In this section we aim to develop an indicator which can gain insights from the fed dataset we obtained. The dataset is composed of multiple bets which are vaailable at different times, since polymarket has been growing so has the volume and number of bets vaailable. We look at the variation in probability for all events. Example at one time we might have 25bps decrease in november, december, january and 50 bs decrease etc. Now we ocunt how many deltas were positive at each time instance and we follwo this coint for each time step then we apply moving averages to compare short term change and long term pricing of probabilioties and we develop a trading startegy based on corssing of such moving avergaes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726186d1",
   "metadata": {},
   "source": [
    "## 4. Fed Market Sentiment Indicator — Construction, Logic & Interpretation\n",
    "\n",
    "This section introduces a **market-implied sentiment indicator** based on Polymarket probabilities for Federal Reserve interest rate outcomes.  \n",
    "The goal is to capture shifts in expectations — *how dovish or hawkish the market is becoming* — and study how these shifts relate to the S&P 500.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Background and Motivation\n",
    "\n",
    "Polymarket lists multiple prediction markets for each Fed meeting — for example:\n",
    "- `..._25_bps_decrease`\n",
    "- `..._50_bps_decrease`\n",
    "- `..._No_Change`\n",
    "- `..._25_bps_increase`\n",
    "\n",
    "Each column represents the **probability** that a given rate outcome will occur.  \n",
    "As macroeconomic data, speeches, or risk events unfold, these probabilities move — revealing the market’s evolving beliefs about monetary policy.\n",
    "\n",
    "Our goal is to summarize these changes into one interpretable metric — the **Net BPS Sentiment Indicator** — which measures whether the overall expectation is shifting *dovish* (toward cuts) or *hawkish* (toward hikes).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2. Step-by-Step Construction\n",
    "\n",
    "At each time step \\( t \\) and for each outcome \\( j \\):\n",
    "\n",
    "1. **Compute the change in probability**  \n",
    "   \\[\n",
    "   \\Delta p_{t,j} = p_{t,j} - p_{t-1,j}\n",
    "   \\]\n",
    "   A positive \\(\\Delta p_{t,j}\\) means the market is assigning more likelihood to that outcome today than yesterday.\n",
    "\n",
    "2. **Assign a directional sign (economic interpretation)**  \n",
    "   Each contract is labeled as either **dovish** or **hawkish**:\n",
    "   \\[\n",
    "   w_j =\n",
    "   \\begin{cases}\n",
    "   +1, & \\text{if the outcome is dovish (rate cut)} \\\\\n",
    "   -1, & \\text{if the outcome is hawkish (rate hike or no change)}\n",
    "   \\end{cases}\n",
    "   \\]\n",
    "\n",
    "   - A rise in the probability of a **rate cut** → dovish (+).  \n",
    "   - A rise in the probability of a **rate hike or no change** → hawkish (–).\n",
    "\n",
    "3. **Compute the Net Sentiment Count**\n",
    "   \\[\n",
    "   S_t = \\sum_j w_j \\cdot \\text{sign}(\\Delta p_{t,j})\n",
    "   \\]\n",
    "\n",
    "   - \\( S_t > 0 \\): the market shifted dovish (pricing more cuts).  \n",
    "   - \\( S_t < 0 \\): the market shifted hawkish (pricing fewer cuts or hikes).\n",
    "\n",
    "4. **Apply Smoothing (Moving Averages)**  \n",
    "   Since \\(S_t\\) fluctuates rapidly, we apply:\n",
    "   - **5-day MA:** captures short-term sentiment shifts.\n",
    "   - **10-day MA:** captures the medium-term trend.\n",
    "\n",
    "   The crossover between these smoothed signals helps identify persistent turning points in monetary sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3. Why the Indicator Oscillates\n",
    "\n",
    "The raw \\( S_t \\) often swings sharply. This is expected because:\n",
    "\n",
    "- **Contracts are interdependent.**  \n",
    "  If the “25 bps cut” probability rises, the “no change” and “25 bps hike” probabilities must fall.\n",
    "- **Market depth evolves.**  \n",
    "  As Polymarket grows, more events and meetings appear, increasing the count of active contracts.\n",
    "- **Data updates asynchronously.**  \n",
    "  Not all markets update simultaneously, introducing short-term noise.\n",
    "\n",
    "To filter this, we smooth \\( S_t \\) with moving averages, allowing focus on *persistent directional shifts* rather than micro movements.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4. Interpreting the Chart\n",
    "\n",
    "Below, we plot the **S&P 500** (left axis, blue) against the **Net BPS Sentiment Indicator** (right axis, red):\n",
    "\n",
    "- **Blue line:** S&P 500 close price.  \n",
    "- **Dashed blue line:** 20-day moving average of S&P 500.  \n",
    "- **Gray line:** Raw sentiment signal \\( S_t \\).  \n",
    "- **Red lines:** 5-day and 10-day moving averages of \\( S_t \\).\n",
    "\n",
    "**Interpretation:**\n",
    "- Rising red lines → *Dovish momentum* — markets pricing more cuts or looser policy.  \n",
    "- Falling red lines → *Hawkish momentum* — markets pricing hikes or fewer cuts.  \n",
    "- Dovish phases often align with **rising equities**, while hawkish phases coincide with **equity pullbacks**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5. Economic Intuition\n",
    "\n",
    "The indicator captures the **direction of probability flow** among rate outcomes — essentially, how the market is repricing the expected *path* of policy rates.  \n",
    "\n",
    "- When \\( S_t > 0 \\), traders are collectively shifting toward **easier monetary policy expectations**.  \n",
    "- When \\( S_t < 0 \\), expectations are tightening, signaling **higher rates or delayed cuts**.\n",
    "\n",
    "This makes the indicator a **forward-looking macro sentiment index** — derived not from opinions or news, but from *real-money pricing of monetary policy expectations*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-PortfolioManagement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
